{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import time\n",
    "from shapely.geometry import Polygon\n",
    "import os\n",
    "import os.path as osp\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph_shapefile_directional(name_file, G, filepath=None, encoding=\"utf-8\"):\n",
    "    # default filepath if none was provided\n",
    "    if filepath is None:\n",
    "        filepath = os.path.join(ox.settings.data_folder, \"graph_shapefile\")\n",
    "\n",
    "    # if save folder does not already exist, create it (shapefiles\n",
    "    # get saved as set of files)\n",
    "    if not filepath == \"\" and not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    filepath_nodes = os.path.join(filepath, \"nodes.shp\")\n",
    "    filepath_edges = os.path.join(filepath, \"edges.shp\")\n",
    "\n",
    "\n",
    "    # convert undirected graph to gdfs and stringify non-numeric columns\n",
    "    gdf_nodes, gdf_edges = ox.utils_graph.graph_to_gdfs(G)\n",
    "    gdf_nodes = ox.io._stringify_nonnumeric_cols(gdf_nodes)\n",
    "    gdf_edges = ox.io._stringify_nonnumeric_cols(gdf_edges)\n",
    "    # We need an unique ID for each edge\n",
    "    # filtramos calles\n",
    "    # gdf_edges = gdf_edges[gdf_edges[\"highway\"]!='living_street']\n",
    "    # gdf_edges = gdf_edges[gdf_edges[\"highway\"]!='residential']\n",
    "    # gdf_edges = gdf_edges[gdf_edges[\"highway\"]!=\"['living_street', 'residential']\"]\n",
    "    # gdf_edges = gdf_edges[gdf_edges[\"highway\"]!=\"['residential', 'living_street']\"]\n",
    "    # ids = [i + 1 for i in range(len(gdf_edges.index))]\n",
    "    # gdf_edges[\"id\"] = ids\n",
    "    gdf_edges[\"fid\"] = np.arange(0, gdf_edges.shape[0], dtype='int')\n",
    "\n",
    "    # save the nodes and edges as separate ESRI shapefiles\n",
    "    gdf_nodes.to_file(filepath_nodes, encoding=encoding)\n",
    "    gdf_edges.to_file(filepath_edges, encoding=encoding)\n",
    "\n",
    "    # a = gdf_nodes.to_json()\n",
    "    # b = gdf_edges.to_json()\n",
    "    #\n",
    "    # print(gdf_nodes.columns)\n",
    "    # print(gdf_nodes.head())\n",
    "    # print(gdf_edges.columns)\n",
    "    # print(gdf_edges.head())\n",
    "\n",
    "    # archivo nodo to csv\n",
    "    osmid = [i for i in gdf_nodes.index]\n",
    "    y = list(gdf_nodes['y'])\n",
    "    x = list(gdf_nodes['x'])\n",
    "    street_count = list(gdf_nodes['street_count'])\n",
    "    highway = list(gdf_nodes['highway'])\n",
    "    geometry = list(gdf_nodes['geometry'])\n",
    "\n",
    "    with open(os.path.join(\"osm/{}/{}_nodos_completo.csv\".format(name_file, name_file)), \"w\", newline=\"\",\n",
    "              encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f, delimiter=\",\")\n",
    "        writer.writerow([\"osmid\", \"y\", \"x\", \"street_count\", \"highway\", \"geometry\"])\n",
    "\n",
    "        for i in range(len(list(gdf_nodes['y']))):\n",
    "            writer.writerow([osmid[i], y[i], x[i], street_count[i], highway[i], geometry[i]])\n",
    "\n",
    "    # archivo arco to csv\n",
    "    fid = np.arange(0, gdf_edges.shape[0], dtype='int')\n",
    "    osmid = list(gdf_edges['osmid'])\n",
    "    u = [u for u, v, key in gdf_edges.index]\n",
    "    v = [v for u, v, key in gdf_edges.index]\n",
    "    key = [key for u, v, key in gdf_edges.index]\n",
    "    highway = list(gdf_edges['highway'])\n",
    "    oneway = list(gdf_edges['oneway'])\n",
    "    length = list(gdf_edges['length'])\n",
    "    geometry = list(gdf_edges['geometry'])\n",
    "    name = list(gdf_edges['name'])\n",
    "    lanes = list(gdf_edges['lanes'])\n",
    "    maxspeed = list(gdf_edges['maxspeed'])\n",
    "\n",
    "    with open(os.path.join(\"osm/{}/{}_arcos_completo.csv\".format(name_file, name_file)), \"w\", newline=\"\",\n",
    "              encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f, delimiter=\",\")\n",
    "        writer.writerow(\n",
    "            [\"fid\", \"osmid\", \"osmid_nodo_origen\", \"osmid_nodo_destino\", \"key\", \"highway\", \"oneway\", \"length\",\n",
    "             \"geometry\",\n",
    "             \"name\", \"lanes\", \"maxspeed\"])\n",
    "\n",
    "        for i in range(len(list(gdf_edges['osmid']))):\n",
    "            writer.writerow(\n",
    "                [fid[i], osmid[i], u[i], v[i], key[i], highway[i], oneway[i], length[i], geometry[i], name[i], lanes[i],\n",
    "                 maxspeed[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"osmnx version\", ox.__version__)\n",
    "# Download by a bounding box\n",
    "x1, y1, x2, y2 = 148.9643, -35.4724, 149.2603, -35.1393 # West/South/East/North\n",
    "boundary_polygon = Polygon([(x1, y1), (x2, y1), (x2, y2), (x1, y2)])\n",
    "G = ox.graph_from_polygon(boundary_polygon, network_type='drive')\n",
    "\n",
    "start_time = time.time()\n",
    "save_graph_shapefile_directional('Canberra', G, filepath='osm/Canberra')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the output to simulator format\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TARGET_CITY = 'Canberra'\n",
    "# Load the nodes\n",
    "nodes = pd.read_csv('osm/{}/{}_nodos_completo.csv'.format(TARGET_CITY, TARGET_CITY))\n",
    "\n",
    "# Create the nodes df\n",
    "sim_nodes_df = pd.DataFrame(columns=['node_id', 'longitude', 'latitude', 'zone_id', 'osm_node_id'])\n",
    "\n",
    "# Load the edges\n",
    "edges = pd.read_csv('osm/{}/{}_arcos_completo.csv'.format(TARGET_CITY, TARGET_CITY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the nodes\n",
    "sim_nodes_df['osm_node_id'] = nodes['osmid']\n",
    "sim_nodes_df['longitude'] = nodes['x']\n",
    "sim_nodes_df['latitude'] = nodes['y']\n",
    "sim_nodes_df['node_id'] = np.arange(1, len(sim_nodes_df) + 1)\n",
    "sim_nodes_df['zone_id'] = 0\n",
    "\n",
    "sim_nodes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the edges\n",
    "edges.head()\n",
    "\n",
    "sim_edge_df = pd.DataFrame(columns=['edge_id', 'from_node_id', 'to_node_id', 'length', 'max_speed', 'edge_type', 'oneway', 'osm_way_id'])\n",
    "\n",
    "sim_edge_df['from_node_id'] = edges['osmid_nodo_origen']\n",
    "sim_edge_df['to_node_id'] = edges['osmid_nodo_destino']\n",
    "sim_edge_df['length'] = edges['length']\n",
    "sim_edge_df['max_speed'] = edges['maxspeed']\n",
    "sim_edge_df['edge_type'] = edges['highway']\n",
    "sim_edge_df['oneway'] = edges['oneway']\n",
    "sim_edge_df['osm_way_id'] = edges['osmid']\n",
    "sim_edge_df['edge_id'] = np.arange(1, len(sim_edge_df) + 1)\n",
    "\n",
    "sim_edge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Mapping the node_id in edges to the new node_id\n",
    "\n",
    "sim_edge_df['from_node_id'] = sim_edge_df['from_node_id'].map(sim_nodes_df.set_index('osm_node_id')['node_id'])\n",
    "sim_edge_df['to_node_id'] = sim_edge_df['to_node_id'].map(sim_nodes_df.set_index('osm_node_id')['node_id'])\n",
    "\n",
    "sim_edge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Duplicate the edges which are not one way, with reversed direction\n",
    "sim_edge_df_reverse = sim_edge_df.copy()\n",
    "sim_edge_df_reverse['from_node_id'] = sim_edge_df['to_node_id']\n",
    "sim_edge_df_reverse['to_node_id'] = sim_edge_df['from_node_id']\n",
    "# delete the oneway edges\n",
    "sim_edge_df_reverse = sim_edge_df_reverse[sim_edge_df_reverse['oneway'] == False]\n",
    "# reset the oneway\n",
    "sim_edge_df_reverse['oneway'] = True\n",
    "# reset the edge_id\n",
    "sim_edge_df_reverse['edge_id'] = np.arange(len(sim_edge_df) + 1, len(sim_edge_df) + 1 + len(sim_edge_df_reverse))\n",
    "# cat the two edges\n",
    "sim_edge_df = pd.concat([sim_edge_df, sim_edge_df_reverse], axis=0)\n",
    "\n",
    "sim_edge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Assign Traverse Time according to edge_type\n",
    "speed_dict = {\n",
    "    # 主干道\n",
    "    'motorway': 110,          # 高速公路\n",
    "    'motorway_link': 60,      # 高速公路匝道\n",
    "    'trunk': 80,              # 城市快速路\n",
    "    'trunk_link': 50,         # 快速路匝道\n",
    "    \n",
    "    # 城市主要道路\n",
    "    'primary': 70,            # 城市主干道\n",
    "    'primary_link': 50,       # 主干道匝道\n",
    "    'secondary': 60,          # 城市次干道\n",
    "    'secondary_link': 40,     # 次干道匝道\n",
    "    'tertiary': 50,          # 三级道路\n",
    "    'tertiary_link': 40,     # 三级道路匝道\n",
    "    \n",
    "    # 次要道路\n",
    "    'residential': 40,        # 居住区道路\n",
    "    'living_street': 30,      # 生活街区\n",
    "    'unclassified': 40,      # 未分类道路\n",
    "    'busway': 50,            # 公交专用道\n",
    "    \n",
    "    # 组合道路类型(取较高速度限制)\n",
    "    \"['motorway', 'trunk']\": 110,\n",
    "    \"['motorway', 'primary']\": 110,\n",
    "    \"['primary_link', 'residential']\": 50,\n",
    "    \"['living_street', 'tertiary']\": 50,\n",
    "    \"['residential', 'unclassified']\": 40,\n",
    "    \"['living_street', 'unclassified']\": 40,\n",
    "    \"['secondary', 'residential']\": 60,\n",
    "    \"['secondary', 'unclassified']\": 60,\n",
    "    \"['tertiary', 'residential']\": 50,\n",
    "    \"['residential', 'tertiary']\": 50,\n",
    "    \"['primary_link', 'secondary_link']\": 50,\n",
    "    \"['primary_link', 'unclassified']\": 50,\n",
    "    \"['secondary', 'secondary_link']\": 60,\n",
    "    \"['secondary', 'tertiary']\": 60\n",
    "}\n",
    "\n",
    "sim_edge_df['edge_time'] = (sim_edge_df['length']/1000)/sim_edge_df['edge_type'].map(speed_dict)*3600 # in seconds\n",
    "# round to seconds\n",
    "sim_edge_df['edge_time'] = sim_edge_df['edge_time'].round(0)\n",
    "sim_edge_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Clean the Edge Dataframe\n",
    "sim_edge_df_clean = sim_edge_df[['edge_id', 'from_node_id', 'to_node_id', 'edge_time']]\n",
    "sim_edge_df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Find isolated nodes and remove them\n",
    "sim_nodes_df_clean = sim_nodes_df.copy()\n",
    "sim_nodes_df_clean = sim_nodes_df_clean[sim_nodes_df_clean['node_id'].isin(sim_edge_df_clean['from_node_id']) | sim_nodes_df_clean['node_id'].isin(sim_edge_df_clean['to_node_id'])]\n",
    "print(\"num of isolated nodes: \", len(sim_nodes_df) - len(sim_nodes_df_clean))\n",
    "sim_nodes_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Check if all the nodes in the edge dataframe are in the node dataframe\n",
    "reduntant_nodes = sim_edge_df_clean[~sim_edge_df_clean['from_node_id'].isin(sim_nodes_df_clean['node_id']) | ~sim_edge_df_clean['to_node_id'].isin(sim_nodes_df_clean['node_id'])]\n",
    "print(\"num of reduntant nodes: \", len(reduntant_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Use K-Means to cluster the nodes\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_ZONES = 32\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# sim_nodes_df_clean[['longitude', 'latitude']] = scaler.fit_transform(sim_nodes_df_clean[['longitude', 'latitude']])\n",
    "kmeans = KMeans(n_clusters=NUM_ZONES, random_state=0).fit(sim_nodes_df_clean[['longitude', 'latitude']])\n",
    "sim_nodes_df_clean['zone_id'] = kmeans.labels_\n",
    "\n",
    "# 可视化聚类结果，不包括孤立节点\n",
    "plt.scatter(sim_nodes_df_clean['latitude'], sim_nodes_df_clean['longitude'], \n",
    "            c=sim_nodes_df_clean['zone_id'], cmap='viridis', s= 5)\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.title('Manhattan - Zones')\n",
    "plt.colorbar(label='Zone')\n",
    "# plt.savefig(osp.join(BASEDIR, OUTPUT_NAME + '_Node_Clustering.png'),dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sim_nodes_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Build Graph, build area adjacency matrix\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ADJ MATRIX on ZONES\n",
    "# 创建网络图，并添加所有节点和边\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 添加所有节点\n",
    "for node_id in sim_nodes_df_clean['node_id']:\n",
    "    G.add_node(node_id)\n",
    "    # add latitude and longitude\n",
    "    G.nodes[node_id]['latitude'] = sim_nodes_df_clean.loc[sim_nodes_df_clean['node_id'] == node_id, 'latitude'].values[0]\n",
    "    G.nodes[node_id]['longitude'] = sim_nodes_df_clean.loc[sim_nodes_df_clean['node_id'] == node_id, 'longitude'].values[0]\n",
    "    G.nodes[node_id]['zone_id'] = sim_nodes_df_clean.loc[sim_nodes_df_clean['node_id'] == node_id, 'zone_id'].values[0]\n",
    "\n",
    "# 添加边\n",
    "num_edges = len(sim_edge_df_clean)\n",
    "arcs = tqdm(sim_edge_df_clean.iterrows(), total=num_edges, ncols=100, desc='Building network...')\n",
    "for idx, arc in arcs:\n",
    "    Oid = arc['from_node_id']\n",
    "    Did = arc['to_node_id']\n",
    "    TimeCost = arc['edge_time']\n",
    "    G.add_edge(Oid, Did, TimeCost=TimeCost)\n",
    "\n",
    "print(f'Network has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.')\n",
    "\n",
    "# 初始化邻接矩阵\n",
    "area_adjacency_matrix = np.zeros((NUM_ZONES, NUM_ZONES), dtype=int)\n",
    "\n",
    "# 检查邻接关系\n",
    "for node in sim_nodes_df_clean['node_id']:\n",
    "    node_zone = sim_nodes_df_clean.loc[sim_nodes_df_clean['node_id'] == node, 'zone_id'].values[0]\n",
    "    neighbors = list(G.neighbors(node))\n",
    "\n",
    "    for neighbor in neighbors:\n",
    "        neighbor_zone = sim_nodes_df_clean.loc[sim_nodes_df_clean['node_id'] == neighbor, 'zone_id'].values[0]\n",
    "\n",
    "        # 标记不同区域间的直接相邻关系\n",
    "        if node_zone != neighbor_zone and neighbor_zone != -1:\n",
    "            area_adjacency_matrix[node_zone, neighbor_zone] = 1\n",
    "            area_adjacency_matrix[neighbor_zone, node_zone] = 1  # 确保对称\n",
    "\n",
    "# 添加自环\n",
    "np.fill_diagonal(area_adjacency_matrix, 1)\n",
    "\n",
    "# 输出并保存邻接矩阵\n",
    "print(\"Area Adjacency Matrix:\")\n",
    "print(area_adjacency_matrix)\n",
    "\n",
    "# # 保存邻接矩阵到CSV\n",
    "# np.savetxt(osp.join(BASEDIR, OUTPUT_NAME + '_Zone_Adjacency_Matrix.csv'), area_adjacency_matrix, delimiter=\",\", fmt='%d')\n",
    "\n",
    "# pickle保存\n",
    "OUTPUT_PATH = 'osm/' + TARGET_CITY + '/'\n",
    "OUTPUT_NAME = TARGET_CITY + '_Zone_Adjacency_Matrix.pickle'\n",
    "with open(osp.join(OUTPUT_PATH + OUTPUT_NAME), 'wb') as f:\n",
    "    pickle.dump(area_adjacency_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11.1: Check the nodes in visualization\n",
    "\n",
    "# Create Area adj map \n",
    "import matplotlib.colors as mcolors \n",
    "import folium \n",
    "import numpy as np \n",
    "from sklearn.cluster import KMeans \n",
    "import colorsys \n",
    "import pandas as pd \n",
    "# # 准备数据为KMeans聚类 \n",
    "# node_coordinates = np.array([[G.nodes[node]['latitude'], G.nodes[node]['longitude']] for node in G.nodes]) \n",
    "# # 执行KMeans聚类 \n",
    "# k = NUM_ZONES\n",
    "# kmeans = KMeans(n_clusters=k, random_state=0) \n",
    "# cluster_labels = kmeans.labels_\n",
    "# # 为每个节点添加zone_id属性并创建DataFrame \n",
    "# nodes_data = [] \n",
    "# for node, label in zip(G.nodes, cluster_labels): \n",
    "#     G.nodes[node]['zone_id'] = int(label) \n",
    "\n",
    "# 创建高饱和度的颜色映射 \n",
    "def generate_distinct_colors(n): \n",
    "    colors = [] \n",
    "    for i in range(n): \n",
    "        hue = i / n \n",
    "        saturation = 0.8 + (i % 3) * 0.1 \n",
    "        value = 0.7 + (i % 2) * 0.2 \n",
    "        rgb = colorsys.hsv_to_rgb(hue, saturation, value) \n",
    "        colors.append(mcolors.rgb2hex(rgb)) \n",
    "    return colors \n",
    "\n",
    "colors = generate_distinct_colors(k) \n",
    "\n",
    "# 创建Folium地图对象 \n",
    "center_latitude = np.nanmean([G.nodes[node]['latitude'] for node in G.nodes])\n",
    "center_longitude = np.nanmean([G.nodes[node]['longitude'] for node in G.nodes])\n",
    "m = folium.Map(location=[center_latitude, center_longitude], zoom_start=12, tiles='OpenStreetMap') \n",
    "\n",
    "# All the edges\n",
    "for edge in G.edges():\n",
    "    node1, node2 = edge\n",
    "    coordinates = [(G.nodes[node1]['latitude'], G.nodes[node1]['longitude']), (G.nodes[node2]['latitude'], G.nodes[node2]['longitude'])]\n",
    "    folium.PolyLine(locations=coordinates, color='grey', weight=2, opacity=1).add_to(m)\n",
    "\n",
    "# All the nodes\n",
    "for node in G.nodes():\n",
    "    zone_id = G.nodes[node]['zone_id']\n",
    "    color = colors[zone_id]\n",
    "\n",
    "    folium.CircleMarker(location=(G.nodes[node]['latitude'], G.nodes[node]['longitude']), radius=3, color=color, fill=True, fill_color=color, fill_opacity=1, weight=2, popup=f'Zone: {zone_id}').add_to(m)\n",
    "\n",
    "# 保存地图 \n",
    "OUTPUT_PATH = 'osm/' + TARGET_CITY + '/'\n",
    "OUTPUT_NAME = TARGET_CITY + '_map_nodes.html'\n",
    "m.save(osp.join(OUTPUT_PATH, OUTPUT_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11.2: Check the zones in visualization\n",
    "\n",
    "# Create Area adj map \n",
    "import matplotlib.colors as mcolors \n",
    "import folium \n",
    "import numpy as np \n",
    "from sklearn.cluster import KMeans \n",
    "import colorsys \n",
    "import pandas as pd \n",
    "\n",
    "nodes_df = sim_nodes_df_clean.copy() \n",
    "k = NUM_ZONES\n",
    "# 计算每个zone的质心 \n",
    "centroids = nodes_df.groupby('zone_id')[['latitude', 'longitude']].mean() \n",
    "# 创建高饱和度的颜色映射 \n",
    "def generate_distinct_colors(n): \n",
    "    colors = [] \n",
    "    for i in range(n): \n",
    "        hue = i / n \n",
    "        saturation = 0.8 + (i % 3) * 0.1 \n",
    "        value = 0.7 + (i % 2) * 0.2 \n",
    "        rgb = colorsys.hsv_to_rgb(hue, saturation, value) \n",
    "        colors.append(mcolors.rgb2hex(rgb)) \n",
    "    return colors \n",
    "\n",
    "colors = generate_distinct_colors(k) \n",
    "\n",
    "# 创建Folium地图对象 \n",
    "center_latitude = np.nanmean(centroids['latitude']) \n",
    "center_longitude = np.nanmean(centroids['longitude']) \n",
    "m = folium.Map(location=[center_latitude, center_longitude], zoom_start=12, tiles='OpenStreetMap') \n",
    "\n",
    "# 根据邻接矩阵绘制相邻区域之间的连线 \n",
    "for i in range(k): \n",
    "    for j in range(i+1, k): \n",
    "        # 避免重复绘制对称线 \n",
    "        if area_adjacency_matrix[i, j] == 1: \n",
    "            # 只画相邻区域之间的连线 \n",
    "            coord1 = [centroids.loc[i, 'latitude'], centroids.loc[i, 'longitude']] \n",
    "            coord2 = [centroids.loc[j, 'latitude'], centroids.loc[j, 'longitude']] \n",
    "            folium.PolyLine( locations=[coord1, coord2], weight=5, color='gray', opacity=1 ).add_to(m) \n",
    "# 绘制质心点 \n",
    "for zone_id in range(k): \n",
    "    color = colors[zone_id] \n",
    "    folium.CircleMarker( location=[centroids.loc[zone_id, 'latitude'], \n",
    "                        centroids.loc[zone_id, 'longitude']], radius=15, color=color, fill=True, fill_color=color, fill_opacity=1, weight=2, popup=f'Zone: {zone_id}' ).add_to(m) \n",
    "# 保存地图 \n",
    "OUTPUT_PATH = 'osm/' + TARGET_CITY + '/'\n",
    "OUTPUT_NAME = TARGET_CITY + '_map_zones_centroids.html'\n",
    "m.save(osp.join(OUTPUT_PATH, OUTPUT_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = nx.single_source_dijkstra_path(G, 1, weight='TimeCost')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Calculate All Path Matrix\n",
    "# Use Graph Dijistra to calculate all path matrix\n",
    "\n",
    "# initialize the all path matrix\n",
    "# start index is zero, but node index starts from 1, so we need to add one more row and column\n",
    "all_path_matrix = np.zeros((G.number_of_nodes()+1, G.number_of_nodes()+1), dtype=int)\n",
    "\n",
    "# calculate all path matrix\n",
    "for node_id in tqdm(range(1, G.number_of_nodes()+1, 1), ncols=100, desc='Calculating all path matrix...'):\n",
    "    paths = nx.single_source_dijkstra_path(G, node_id, weight='TimeCost')\n",
    "    for target_node, path in paths.items():\n",
    "        for step_idx in range(len(path)):\n",
    "            if len(path) == 1:\n",
    "                all_path_matrix[path[step_idx], target_node] = path[step_idx]\n",
    "                break\n",
    "            current_node = path[step_idx]\n",
    "            next_node = path[step_idx+1]\n",
    "            all_path_matrix[int(current_node), int(target_node)] = int(next_node)\n",
    "            if next_node == target_node:\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Calculate All Path Matrix with GPU Acceleration(CUDA)\n",
    "import cudf\n",
    "import cugraph\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Initialize the all path matrix\n",
    "# Start index is zero, but node index starts from 1\n",
    "all_path_matrix = np.zeros((G.number_of_nodes()+1, G.number_of_nodes()+1), dtype=int)\n",
    "\n",
    "# Convert NetworkX graph to edge list for cugraph\n",
    "print(\"Converting graph to edge list...\")\n",
    "edges = [(u, v, d['TimeCost']) for u, v, d in G.edges(data=True)]\n",
    "df = pd.DataFrame(edges, columns=['source', 'destination', 'weight'])\n",
    "\n",
    "# Create CuGraph object\n",
    "print(\"Creating CuGraph object...\")\n",
    "G_cu = cugraph.Graph()\n",
    "G_cu.from_pandas_edgelist(\n",
    "    df,\n",
    "    source='source',\n",
    "    destination='destination',\n",
    "    edge_attr='weight',\n",
    "    renumber=False  # Keep original node IDs\n",
    ")\n",
    "\n",
    "# Calculate all path matrix using GPU\n",
    "print(\"Calculating paths...\")\n",
    "for node_id in tqdm(range(1, G.number_of_nodes()+1), ncols=100, desc='Calculating all path matrix...'):\n",
    "    # Calculate shortest paths from source to all other nodes\n",
    "    paths = cugraph.sssp(G_cu, node_id)\n",
    "    paths_pd = paths.to_pandas()\n",
    "    \n",
    "    # Process each target node\n",
    "    for _, row in paths_pd.iterrows():\n",
    "        target_node = int(row['vertex'])\n",
    "        if row['distance'] != np.inf:  # If path exists\n",
    "            # Get the path using predecessor map\n",
    "            path = cugraph.utils.get_path_from_predecessor(paths, node_id, target_node)\n",
    "            \n",
    "            # Update the all path matrix\n",
    "            for step_idx in range(len(path)-1):\n",
    "                current_node = int(path[step_idx])\n",
    "                next_node = int(path[step_idx+1])\n",
    "                all_path_matrix[current_node, target_node] = next_node\n",
    "                if next_node == target_node:\n",
    "                    break\n",
    "\n",
    "# Save the results using both numpy and pickle\n",
    "print(\"Saving results...\")\n",
    "np.save('all_path_matrix_gpu.npy', all_path_matrix)\n",
    "with open('all_path_matrix_gpu.pkl', 'wb') as f:\n",
    "    pickle.dump(all_path_matrix, f)\n",
    "\n",
    "# Optional: Verify saving worked\n",
    "print(\"Verifying saved files...\")\n",
    "# Verify numpy save\n",
    "loaded_np = np.load('all_path_matrix_gpu.npy')\n",
    "print(f\"Numpy save verification: shapes match = {loaded_np.shape == all_path_matrix.shape}\")\n",
    "\n",
    "# Verify pickle save \n",
    "with open('all_path_matrix_gpu.pkl', 'rb') as f:\n",
    "    loaded_pickle = pickle.load(f)\n",
    "print(f\"Pickle save verification: shapes match = {loaded_pickle.shape == all_path_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Calculate All Path Time Matrix\n",
    "# Creat dijkstra_V matrix\n",
    "# index start from 1\n",
    "dijkstra_V = pd.DataFrame(index=sim_nodes_df_clean['node_id'], columns=sim_nodes_df_clean['node_id'])\n",
    "# Fill the matrix with time cost\n",
    "for i in tqdm(sim_nodes_df_clean['node_id']):\n",
    "    for j in sim_nodes_df_clean['node_id']:\n",
    "        try:\n",
    "            dijkstra_V.loc[i, j] = nx.shortest_path_length(G, i, j, weight='TimeCost')\n",
    "        except:\n",
    "            dijkstra_V.loc[i, j] = np.inf\n",
    "\n",
    "# Save all path time matrix (Efficient!)\n",
    "map_dijkstra_V = copy.deepcopy(dijkstra_V).T\n",
    "# add 1 to column and row\n",
    "map_dijkstra_V.index = dijkstra_V.index + 1\n",
    "map_dijkstra_V.columns = dijkstra_V.columns + 1\n",
    "\n",
    "# with open(osp.join(BASEDIR,OUTPUT_NAME + '_AllPathTimeMatrix.pickle'), 'wb') as f:\n",
    "#      pickle.dump(map_dijkstra_V, f)\n",
    "\n",
    "map_dijkstra_V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elispe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
